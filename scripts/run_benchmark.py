#!/usr/bin/env python3
"""
PDEBench ç»Ÿä¸€è¯„æµ‹å…¥å£

ç”¨æ³•:
    # è¯„æµ‹å•ä¸ªLLM
    python run_benchmark.py --agent gpt-4o
    
    # è¯„æµ‹å¤šä¸ªLLM
    python run_benchmark.py --agent gpt-4o sonnet-3.5 gemini
    
    # åªæµ‹è¯•ç‰¹å®šcases
    python run_benchmark.py --agent gpt-4o --cases poisson_basic heat_basic
    
    # åªæµ‹è¯•ç‰¹å®šæ–¹ç¨‹ç±»å‹
    python run_benchmark.py --agent gpt-4o --equation-types poisson heat
    
    # è·³è¿‡LLMè°ƒç”¨ï¼Œåªè¯„æµ‹å·²æœ‰solver
    python run_benchmark.py --agent gpt-4o --skip-generation
    
    # ä½¿ç”¨å·²æœ‰solver.py
    python run_benchmark.py --agent gpt-4o --solver-path /Users/yusan/agent/pdebench/results/gpt-5.1/poisson_basic/solver.py --cases poisson_basic

æµç¨‹:
    1. ä» data/benchmark.jsonl åŠ è½½cases
    2. å¯¹æ¯ä¸ªcase:
       a. è¿è¡Œoracleè·å–å‚è€ƒè§£ï¼ˆå¸¦ç¼“å­˜ï¼‰
       b. ç”Ÿæˆprompt
       c. è°ƒç”¨LLMç”Ÿæˆsolverä»£ç 
       d. æ‰§è¡Œsolverï¼Œè®¡ç®—è¯¯å·®
       e. å•æ¡£é€šè¿‡ç‡è¯„æµ‹ï¼ˆç²¾åº¦â†’æ—¶é—´ï¼‰
    3. æ±‡æ€»ç»“æœï¼Œä¿å­˜æŠ¥å‘Š
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
import numpy as np

# æ·»åŠ pdebenchåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from pdebench.core.prompt_builder import generate_prompt
from pdebench.core.llm_client import call_llm, LLMClient
from pdebench.metrics.specialized import get_specialized_metrics_computer


# =============================================================================
# æ•°æ®åŠ è½½
# =============================================================================

def load_benchmark_cases(
    data_file: Path,
    case_filter: Optional[List[str]] = None,
    equation_types: Optional[List[str]] = None
) -> List[Dict]:
    """ä»benchmark.jsonlåŠ è½½cases"""
    cases = []
    eq_types = [t.lower() for t in equation_types] if equation_types else None
    with open(data_file) as f:
        for line in f:
            if line.strip():
                case = json.loads(line)
                if case_filter is not None and case['id'] not in case_filter:
                    continue
                if eq_types is not None:
                    pde_type = case.get('oracle_config', {}).get('pde', {}).get('type', '').lower()
                    if pde_type not in eq_types:
                        continue
                cases.append(case)
    return cases


# =============================================================================
# Oracleæ±‚è§£å™¨ (v2 - ç»Ÿä¸€å…¥å£)
# =============================================================================

def run_oracle(case: Dict, cache_dir: Path) -> Dict:
    """
    è¿è¡Œ Oracle æ±‚è§£å™¨è·å– baselineï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    ä½¿ç”¨ç»Ÿä¸€ OracleSolverï¼Œè¾“å‡º L2 reference å’Œå‚è€ƒæ—¶é—´ã€‚
    """
    case_id = case['id']
    cache_file = cache_dir / f"{case_id}.json"
    
    # æ£€æŸ¥ç¼“å­˜
    if cache_file.exists():
        with open(cache_file) as f:
            cached = json.load(f)
        print(f"   âœ… Using cached oracle")
        return cached
    
    print(f"   ğŸ”® Running oracle...")
    
    try:
        from pdebench.oracle import OracleSolver
        
        oracle = OracleSolver()
        oracle_config = case['oracle_config']
        
        # è°ƒç”¨ç»Ÿä¸€ Oracle æ±‚è§£å™¨
        result = oracle.solve(oracle_config)
        
        # æ„å»ºç¼“å­˜æ•°æ®
        cached = {
            'error': result.baseline_error,
            'time': result.baseline_time,
            'case_id': case_id,
            'num_dofs': result.num_dofs,
            'solver_info': result.solver_info,
            # å­˜å‚¨å‚è€ƒè§£ï¼ˆç”¨äºè¯¯å·®è®¡ç®—ï¼‰
            'reference': result.reference.tolist(),
        }
        
        # ä¿å­˜ç¼“å­˜
        cache_dir.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'w') as f:
            json.dump(cached, f, indent=2)
        
        print(f"   âœ… Oracle: error={result.baseline_error:.2e}, time={result.baseline_time:.3f}s")
        return cached
        
    except Exception as e:
        import traceback
        print(f"   âš ï¸  Oracle failed: {e}")
        traceback.print_exc()
        return {'error': 1e-2, 'time': 10.0, 'case_id': case_id, 'reference': None}


# =============================================================================
# æ‰§è¡Œä¸è¯„æµ‹
# =============================================================================

def execute_solver(solver_code: str, case: Dict, output_dir: Path, timeout: int = 300) -> Dict:
    """æ‰§è¡Œsolverå¹¶è¿”å›ç»“æœ"""
    from pdebench.sandbox.executor import execute_agent_function
    
    # ä¿å­˜solverä»£ç 
    solver_path = output_dir / "solver.py"
    solver_path.write_text(solver_code)
    
    agent_output = output_dir / "agent_output"
    agent_output.mkdir(parents=True, exist_ok=True)
    
    # æ‰§è¡Œ
    result = execute_agent_function(
        script_path=solver_path,
        outdir=agent_output,
        case_spec=case,
        timeout_sec=timeout
    )
    
    if not result.success:
        return {
            'success': False,
            'error': None,
            'time': result.t_agent_run,
            'stdout': result.stdout,
            'stderr': result.stderr,
            'error_message': result.error_message
        }
    
    return {
        'success': True,
        'error': None,  # ç¨åè®¡ç®—
        'time': result.t_agent_run,
        'stdout': result.stdout,
        'stderr': result.stderr,
        'agent_output': agent_output
    }


def compute_error(agent_output: Path, oracle_info: Dict) -> float:
    """
    è®¡ç®—ç›¸å¯¹L2è¯¯å·®
    
    Args:
        agent_output: Agent è¾“å‡ºç›®å½•ï¼ˆåŒ…å« solution.npzï¼‰
        oracle_info: Oracle ç»“æœï¼ˆåŒ…å« reference åˆ—è¡¨ï¼‰
    
    Returns:
        ç›¸å¯¹ L2 è¯¯å·®
    """
    try:
        # åŠ è½½ agent è§£
        agent_sol = np.load(agent_output / "solution.npz")
        u_agent = agent_sol['u']
        
        # ä» oracle_info è·å–å‚è€ƒè§£
        if oracle_info.get('reference') is None:
            print(f"   âš ï¸  No reference solution in oracle cache")
            return float('nan')
        
        u_ref = np.array(oracle_info['reference'])
        
        # å¤„ç†å½¢çŠ¶ä¸åŒ¹é…
        if u_agent.shape != u_ref.shape:
            from scipy.ndimage import zoom
            factors = np.array(u_ref.shape) / np.array(u_agent.shape)
            u_agent = zoom(u_agent, factors, order=1)
        
        # è®¡ç®—ç›¸å¯¹L2è¯¯å·®
        diff = u_agent - u_ref
        ref_norm = np.sqrt(np.sum(u_ref**2))
        
        if ref_norm < 1e-15:
            return np.sqrt(np.sum(diff**2))
        
        rel_L2 = np.sqrt(np.sum(diff**2)) / ref_norm
        
        return float(rel_L2)
        
    except Exception as e:
        print(f"   âš ï¸  Error computation failed: {e}")
        return float('nan')


# =============================================================================
# å•Caseæµç¨‹
# =============================================================================

def run_single_case(
    case: Dict,
    agent_name: str,
    output_dir: Path,
    oracle_cache_dir: Path,
    solver_path_override: Optional[Path] = None,
    skip_generation: bool = False,
    timeout: int = 300
) -> Dict:
    """è¿è¡Œå•ä¸ªcaseçš„å®Œæ•´æµç¨‹"""
    
    case_id = case['id']
    case_output = output_dir / case_id
    case_output.mkdir(parents=True, exist_ok=True)
    
    oracle_output = case_output / "oracle_output"
    oracle_output.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ Case: {case_id}")
    print(f"{'='*60}")
    
    # Step 1: è·å–oracleå‚è€ƒè§£
    oracle_info = run_oracle(case, oracle_cache_dir)
    _write_oracle_reference(case, oracle_info, oracle_output)
    
    # Step 2: ç”Ÿæˆprompt
    prompt = generate_prompt(case, oracle_info)
    (case_output / "prompt.md").write_text(prompt)
    
    # Step 3: è°ƒç”¨LLMæˆ–åŠ è½½å·²æœ‰solver
    solver_path = case_output / "solver.py"
    
    if solver_path_override is not None:
        if not solver_path_override.exists():
            return _make_error_result(case_id, 'SOLVER_NOT_FOUND', f"Solver path not found: {solver_path_override}")
        solver_code = solver_path_override.read_text()
    elif skip_generation and solver_path.exists():
        print(f"   â­ï¸  Using existing solver")
        solver_code = solver_path.read_text()
    else:
        print(f"   ğŸ¤– Calling {agent_name}...")
        try:
            response = call_llm(agent_name, prompt)
            
            if not response.success:
                print(f"   âŒ LLM call failed: {response.error}")
                return _make_error_result(case_id, 'LLM_ERROR', response.error)
            
            solver_code = response.code
            (case_output / "llm_response.txt").write_text(response.raw_response)
            
            if response.usage:
                print(f"   ğŸ“Š Tokens: in={response.usage['input_tokens']}, out={response.usage['output_tokens']}")
                
        except Exception as e:
            print(f"   âŒ LLM call failed: {e}")
            return _make_error_result(case_id, 'LLM_ERROR', str(e))
    
    # Step 4: æ‰§è¡Œsolver
    print(f"   ğŸ”§ Executing solver...")
    exec_result = execute_solver(solver_code, case, case_output, timeout)
    
    if not exec_result['success']:
        print(f"   âŒ Execution failed: {exec_result.get('error_message', 'Unknown')[:100]}")
        return _make_error_result(case_id, 'EXECUTION_ERROR', exec_result.get('error_message'), exec_result.get('stderr'))
    
    # Step 5: è®¡ç®—è¯¯å·®
    error = compute_error(exec_result['agent_output'], oracle_info)
    
    if np.isnan(error):
        print(f"   âŒ Error computation failed")
        return _make_error_result(case_id, 'EVALUATION_ERROR', 'Error computation returned NaN')
    
    print(f"   ğŸ“Š Error: {error:.2e}, Time: {exec_result['time']:.3f}s")
    
    # Step 6: å•æ¡£è¯„æµ‹ (ç²¾åº¦ -> æ—¶é—´)
    eval_cfg = case.get('evaluation_config', {})
    legacy_tolerance = eval_cfg.get('tolerance', 1.2)
    accuracy_tolerance = eval_cfg.get('accuracy_tolerance', legacy_tolerance)
    time_tolerance = eval_cfg.get('time_tolerance', legacy_tolerance)
    target_error = oracle_info['error'] * accuracy_tolerance
    target_time = oracle_info['time'] * time_tolerance
    
    if error > target_error:
        status = 'FAIL'
        fail_reason = f"ACCURACY_FAIL: error={error:.2e} > target={target_error:.2e}"
    elif exec_result['time'] > target_time:
        status = 'FAIL'
        fail_reason = f"TIME_FAIL: time={exec_result['time']:.3f}s > target={target_time:.3f}s"
    else:
        status = 'PASS'
        fail_reason = None
    
    print(f"   âœ… Status: {status}")
    
    # ä¿å­˜ç»“æœ
    result = {
        'case_id': case_id,
        'status': status,
        'error': error,
        'time': exec_result['time'],
        'oracle_error': oracle_info['error'],
        'oracle_time': oracle_info['time'],
        'tolerance': legacy_tolerance,
        'accuracy_tolerance': accuracy_tolerance,
        'time_tolerance': time_tolerance,
        'target_error': target_error,
        'target_time': target_time,
        'fail_reason': fail_reason,
    }
    
    # è®¡ç®—å„math_typeå­æ¦œæŒ‡æ ‡
    math_types = case.get('pde_classification', {}).get('math_type', [])
    math_type_metrics = {}
    for mt in math_types:
        computer = get_specialized_metrics_computer(
            mt, exec_result['agent_output'], oracle_output, case
        )
        if computer is None:
            continue
        metrics = computer.compute({
            'runtime_sec': exec_result['time'],
            'error': error,
            'test_params': {}
        })
        math_type_metrics[mt] = metrics
    
    if math_type_metrics:
        result['math_types'] = math_types
        result['math_type_metrics'] = math_type_metrics
    
    with open(case_output / "result.json", 'w') as f:
        json.dump(result, f, indent=2)
    
    return result


def _make_error_result(case_id: str, status: str, error_msg: str, stderr: str = None) -> Dict:
    """åˆ›å»ºé”™è¯¯ç»“æœ"""
    result = {
        'case_id': case_id,
        'status': status,
        'error_message': error_msg
    }
    if stderr:
        result['stderr'] = stderr
    return result


def _write_oracle_reference(case: Dict, oracle_info: Dict, oracle_output: Path):
    """ä¿å­˜oracleå‚è€ƒè§£åˆ°oracle_output"""
    if oracle_info.get('reference') is None:
        return
    try:
        grid_cfg = case['oracle_config']['output']['grid']
        x = np.linspace(grid_cfg['bbox'][0], grid_cfg['bbox'][1], grid_cfg['nx'])
        y = np.linspace(grid_cfg['bbox'][2], grid_cfg['bbox'][3], grid_cfg['ny'])
        u_star = np.array(oracle_info['reference'])
        np.savez(oracle_output / "reference.npz", x=x, y=y, u_star=u_star)
    except Exception as e:
        print(f"   âš ï¸  Failed to write oracle reference: {e}")


# =============================================================================
# ä¸»æµç¨‹
# =============================================================================

def run_benchmark(
    agents: List[str],
    output_dir: Path,
    data_file: Path,
    case_filter: Optional[List[str]] = None,
    equation_types: Optional[List[str]] = None,
    solver_path: Optional[Path] = None,
    skip_generation: bool = False,
    timeout: int = 300
):
    """è¿è¡Œå®Œæ•´benchmark"""
    
    print("\n" + "="*80)
    print("ğŸš€ PDEBench - LLM/Code Agent Evaluation")
    print("="*80)
    print(f"ğŸ“ Data: {data_file}")
    print(f"ğŸ“ Output: {output_dir}")
    print(f"ğŸ¤– Agents: {', '.join(agents)}")
    print(f"â±ï¸  Timeout: {timeout}s")
    print("="*80)
    
    # éªŒè¯agents
    for agent in agents:
        if agent not in LLMClient.SUPPORTED_AGENTS:
            print(f"âŒ Unknown agent: {agent}")
            print(f"   Supported: {list(LLMClient.SUPPORTED_AGENTS.keys())}")
            sys.exit(1)
    
    # åŠ è½½cases
    cases = load_benchmark_cases(data_file, case_filter, equation_types)
    print(f"\nğŸ“Š Loaded {len(cases)} cases")
    
    if not cases:
        print("âŒ No cases found!")
        sys.exit(1)
    
    oracle_cache_dir = output_dir / ".oracle_cache"
    all_results = {}
    
    for agent_name in agents:
        print(f"\n\n{'#'*80}")
        print(f"# Agent: {agent_name}")
        print(f"{'#'*80}")
        
        agent_output = output_dir / agent_name
        agent_results = []
        
        for i, case in enumerate(cases, 1):
            print(f"\n[{i}/{len(cases)}]", end="")
            result = run_single_case(
                case=case,
                agent_name=agent_name,
                output_dir=agent_output,
                oracle_cache_dir=oracle_cache_dir,
                solver_path_override=solver_path,
                skip_generation=skip_generation,
                timeout=timeout
            )
            agent_results.append(result)
        
        # æ±‡æ€»ç»Ÿè®¡
        summary = compute_summary(agent_name, agent_results)
        
        # ä¿å­˜æ±‡æ€»
        with open(agent_output / "summary.json", 'w') as f:
            json.dump(summary, f, indent=2)
        
        all_results[agent_name] = summary
        print_summary(summary)
    
    # ä¿å­˜æ€»æ±‡æ€»
    output_dir.mkdir(parents=True, exist_ok=True)
    with open(output_dir / "all_results.json", 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print("\n" + "="*80)
    print("âœ… Benchmark Complete!")
    print(f"ğŸ“ Results saved to: {output_dir}")
    print("="*80)


def compute_summary(agent_name: str, results: List[Dict]) -> Dict:
    """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
    total = len(results)
    passed = sum(1 for r in results if r.get('status') == 'PASS')
    errors = [r['error'] for r in results if r.get('status') in ['PASS', 'FAIL'] and r.get('error') is not None]
    times = [r['time'] for r in results if r.get('status') in ['PASS', 'FAIL'] and r.get('time') is not None]
    
    # math_type å­æ¦œ
    math_type_summary: Dict[str, Dict[str, Any]] = {}
    for r in results:
        for mt in r.get('math_types', []):
            if mt not in math_type_summary:
                math_type_summary[mt] = {
                    'cases': 0,
                    'passed': 0,
                    'metric_sums': {},
                    'metric_counts': {}
                }
            math_type_summary[mt]['cases'] += 1
            if r.get('status') == 'PASS':
                math_type_summary[mt]['passed'] += 1
            metrics = r.get('math_type_metrics', {}).get(mt, {})
            for k, v in metrics.items():
                if isinstance(v, (int, float)):
                    math_type_summary[mt]['metric_sums'][k] = (
                        math_type_summary[mt]['metric_sums'].get(k, 0.0) + float(v)
                    )
                    math_type_summary[mt]['metric_counts'][k] = (
                        math_type_summary[mt]['metric_counts'].get(k, 0) + 1
                    )
    
    for mt, info in math_type_summary.items():
        info['pass_rate'] = info['passed'] / info['cases'] if info['cases'] > 0 else 0.0
        avg_metrics = {}
        for k, total_val in info['metric_sums'].items():
            count = info['metric_counts'].get(k, 0)
            if count > 0:
                avg_metrics[k] = total_val / count
        info['avg_metrics'] = avg_metrics
        info.pop('metric_sums', None)
        info.pop('metric_counts', None)
    
    return {
        'agent_name': agent_name,
        'timestamp': datetime.now().isoformat(),
        'total_cases': total,
        'passed_cases': passed,
        'pass_rate': passed / total if total > 0 else 0,
        'avg_error': float(np.mean(errors)) if errors else None,
        'avg_time': float(np.mean(times)) if times else None,
        'math_type_summary': math_type_summary,
        'results': results
    }


def print_summary(summary: Dict):
    """æ‰“å°æ±‡æ€»ä¿¡æ¯"""
    print(f"\n{'â”€'*60}")
    print(f"ğŸ“Š Summary: {summary['agent_name']}")
    print(f"{'â”€'*60}")
    print(f"Total Cases: {summary['total_cases']}")
    print(f"Passed: {summary['passed_cases']} ({summary['pass_rate']:.1%})")
    if summary['avg_error'] is not None:
        print(f"Avg Error: {summary['avg_error']:.2e}")
    if summary['avg_time'] is not None:
        print(f"Avg Time: {summary['avg_time']:.3f}s")
    print(f"{'â”€'*60}")


# =============================================================================
# å…¥å£
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='PDEBench LLM/Code Agent Evaluation',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        '--agent', '-a',
        nargs='+',
        required=True,
        help=f"Agent name(s): {list(LLMClient.SUPPORTED_AGENTS.keys())}"
    )
    
    parser.add_argument(
        '--output', '-o',
        type=Path,
        default=Path('results'),
        help='Output directory (default: results/)'
    )
    
    parser.add_argument(
        '--data',
        type=Path,
        default=Path('data/benchmark.jsonl'),
        help='Benchmark data file (default: data/benchmark.jsonl)'
    )
    
    parser.add_argument(
        '--cases',
        nargs='+',
        default=None,
        help='Specific case IDs to run (default: all)'
    )
    
    parser.add_argument(
        '--equation-types',
        nargs='+',
        default=None,
        help='Equation type(s) to run, e.g., poisson heat (default: all)'
    )
    
    parser.add_argument(
        '--skip-generation',
        action='store_true',
        help='Skip LLM generation, use existing solvers'
    )
    
    parser.add_argument(
        '--solver-path',
        type=Path,
        default=None,
        help='Use an existing solver.py instead of LLM generation'
    )
    
    parser.add_argument(
        '--timeout',
        type=int,
        default=300,
        help='Timeout per case in seconds (default: 300)'
    )
    
    args = parser.parse_args()
    
    # åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•
    root_dir = Path(__file__).parent.parent
    data_file = root_dir / args.data
    output_dir = root_dir / args.output
    
    if not data_file.exists():
        print(f"âŒ Data file not found: {data_file}")
        sys.exit(1)
    
    run_benchmark(
        agents=args.agent,
        output_dir=output_dir,
        data_file=data_file,
        case_filter=args.cases,
        equation_types=args.equation_types,
        solver_path=args.solver_path,
        skip_generation=args.skip_generation,
        timeout=args.timeout
    )


if __name__ == '__main__':
    main()
